{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../assets/maxsqn.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "着实可以理解为soft actor-critic的离散动作版本,但动作sample使用的是Q_network进行softmax操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, column, batch_size, buffer_size):\n",
    "        self.current_state = np.zeros((buffer_size, column), dtype=np.float32)\n",
    "        self.next_state = np.zeros((buffer_size, column), dtype=np.float32)\n",
    "        init = lambda buffer_size : np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.action = init(buffer_size)\n",
    "        self.reward = init(buffer_size)\n",
    "        self.done = init(buffer_size)\n",
    "        self.buffer_size, self.batch_size = buffer_size, batch_size\n",
    "        self.size, self.current_index = 0, 0\n",
    "    \n",
    "    def store(self, current_state, action, next_state, reward, done):\n",
    "        self.current_state[self.current_index] = current_state\n",
    "        self.action[self.current_index] = action\n",
    "        self.next_state[self.current_index] = next_state\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index + 1) % buffer_size\n",
    "        self.size = min(self.buffer_size, (self.size + 1))\n",
    "    \n",
    "    def sample(self):\n",
    "        index = np.random.choice(self.size, self.batch_size, replace=False)\n",
    "        return dict(current_state = torch.tensor(self.current_state[index],dtype=torch.float).to(device),\n",
    "                    action = torch.tensor(self.action[index],dtype=torch.long).reshape(-1, 1).to(device),\n",
    "                    next_state = torch.tensor(self.next_state[index],dtype=torch.float).to(device),\n",
    "                    reward = torch.tensor(self.reward[index]).reshape(-1, 1).to(device),\n",
    "                    done = torch.tensor(self.done[index]).reshape(-1, 1).to(device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Q_network, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state, alpha):\n",
    "        x = self.linear1(state)\n",
    "        x = self.linear2(x)\n",
    "        value = self.linear3(x)\n",
    "        \n",
    "        log_pi = F.softmax(value/alpha, dim=1)\n",
    "        dist = Categorical(log_pi)\n",
    "        \n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"LunarLander-v2\"\n",
    "env_name = \"CartPole-v0\"\n",
    "algorithm_id = \"maxsqn\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "current_time = time.strftime('%Y-%m-%d_%H:%M:%S',time.localtime(time.time()))\n",
    "ROOT_DIR = \"../running_log//{}/{}/{}\".format(algorithm_id, env_name, current_time)\n",
    "model_dir = os.path.join(ROOT_DIR, \"model\")\n",
    "plot_dir = os.path.join(ROOT_DIR, \"tensorboard\")\n",
    "os.makedirs(model_dir)\n",
    "os.makedirs(plot_dir)\n",
    "writer = SummaryWriter(plot_dir)\n",
    "\n",
    "in_dim = env.observation_space.shape[0]\n",
    "out_dim = env.action_space.n\n",
    "\n",
    "Q_net1 = Q_network(in_dim, out_dim).to(device)\n",
    "Q_net2 = Q_network(in_dim, out_dim).to(device)\n",
    "target_Q_net1 = Q_network(in_dim, out_dim).to(device)\n",
    "target_Q_net2 = Q_network(in_dim, out_dim).to(device)\n",
    "target_Q_net1.load_state_dict(Q_net1.state_dict())\n",
    "target_Q_net2.load_state_dict(Q_net2.state_dict())\n",
    "\n",
    "Q_net1_optimizer = optim.Adam(Q_net1.parameters())\n",
    "Q_net2_optimizer = optim.Adam(Q_net2.parameters())\n",
    "\n",
    "train_episodes = 10000\n",
    "train_steps = 500\n",
    "start_steps = 500\n",
    "alpha = 0.15\n",
    "\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 256\n",
    "replay_buffer = ReplayBuffer(in_dim, batch_size, buffer_size)\n",
    "\n",
    "target_entropy = -1.0\n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device) # tensor([0.], device='cuda:0', requires_grad=True)\n",
    "alpha = torch.exp(log_alpha)\n",
    "alpha_optim = optim.Adam([log_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def smooth_plot(factor, item, plot_decay):\n",
    "    item_x = np.arange(len(item))\n",
    "    item_smooth = [np.mean(item[i:i+factor]) if i > factor else np.mean(item[0:i+1])\n",
    "                  for i in range(len(item))]\n",
    "    for i in range(len(item)// plot_decay):\n",
    "        item_x = item_x[::2]\n",
    "        item_smooth = item_smooth[::2]\n",
    "    return item_x, item_smooth\n",
    "    \n",
    "def plot(episode, rewards, critic_losses, actor_losses, episode_steps):\n",
    "    clear_output(True)\n",
    "    rewards_x, rewards_smooth = smooth_plot(10, rewards, 1000)\n",
    "    critic_losses_x, critic_losses_smooth = smooth_plot(10, critic_losses, 1000)\n",
    "    actor_losses_x, actor_losses_smooth = smooth_plot(10, actor_losses, 1000)\n",
    "    episode_steps_x, episode_steps_smooth = smooth_plot(10, episode_steps, 1000)\n",
    "    \n",
    "    plt.figure(figsize=(18, 16))\n",
    "    plt.subplot(411)\n",
    "    plt.title('episode %s. Reward: %s'%(episode, rewards_smooth[-1]))\n",
    "    plt.plot(rewards, label=\"Rewards\", color='lightsteelblue', linewidth='1')\n",
    "    plt.plot(rewards_x, rewards_smooth, label='Smothed_Rewards', color='darkorange', linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.title('Q_net1_losses') #%s. Losses: %s'%(episode, critic_losses_smooth[-1]))\n",
    "    plt.plot(critic_losses,label=\"Q_net1_losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(critic_losses_x, critic_losses_smooth, \n",
    "             label=\"Smoothed_Q_net1_losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.title('Alpha_losses') #%s. Losses: %s'%(episode, actor_losses_smooth[-1]))\n",
    "    plt.plot(actor_losses,label=\"Alpha_losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(actor_losses_x, actor_losses_smooth, \n",
    "             label=\"Smoothed_Alpha_losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.title('Episode_Steps %s. Steps: %s'%(episode, episode_steps_smooth[-1]))\n",
    "    plt.plot(episode_steps,label=\"Episode_Steps\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(episode_steps_x, episode_steps_smooth, \n",
    "             label=\"Episode_Steps_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqn_train(update_time, gamma=0.99, soft_tau=5e-3):\n",
    "    global alpha\n",
    "    samples = replay_buffer.sample()\n",
    "    state, action, next_state = samples['current_state'], samples['action'], samples['next_state']\n",
    "    reward, done = samples['reward'], samples['done']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        target_dist_1, target_value_1 = target_Q_net1(next_state, alpha)\n",
    "        target_dist_2, target_value_2 = target_Q_net2(next_state, alpha)\n",
    "        target_action_1 = target_dist_1.sample().unsqueeze(1)\n",
    "        target_action_2 = target_dist_2.sample().unsqueeze(1)\n",
    "\n",
    "        entropy_1 = target_dist_1.entropy().unsqueeze(1)\n",
    "        target_value = torch.min(\n",
    "                            torch.gather(target_value_1, 1, target_action_1),\n",
    "                            torch.gather(target_value_2, 1, target_action_2)\n",
    "                        )\n",
    "        target_dist, _ = Q_net1(next_state, alpha)\n",
    "        target_entropy_1 = target_dist.entropy().unsqueeze(1)\n",
    "        target_Q = reward + gamma * (1-done) * (target_value - alpha * target_entropy_1)\n",
    "        \n",
    "    _, value1 = Q_net1(state, alpha)\n",
    "    _, value2 = Q_net2(state, alpha)\n",
    "    Q_value_1 = torch.gather(value1, 1, action)\n",
    "    Q_value_2 = torch.gather(value2, 1, action)\n",
    "    \n",
    "    Q_loss_1 = F.mse_loss(target_Q, Q_value_1)\n",
    "    Q_loss_2 = F.mse_loss(target_Q, Q_value_2)\n",
    "    \n",
    "    Q_net1_optimizer.zero_grad()\n",
    "    Q_loss_1.backward()\n",
    "    Q_net1_optimizer.step()\n",
    "    \n",
    "    Q_net2_optimizer.zero_grad()\n",
    "    Q_loss_2.backward()\n",
    "    Q_net2_optimizer.step()\n",
    "    \n",
    "    alpha_loss = - log_alpha * (entropy_1 + target_entropy).mean()\n",
    "\n",
    "    alpha_optim.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    alpha_optim.step()\n",
    "    \n",
    "    alpha = log_alpha.exp()\n",
    "    \n",
    "    for target_param, param in zip(target_Q_net1.parameters(), Q_net1.parameters()):\n",
    "        target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data*soft_tau)\n",
    "    for target_param, param in zip(target_Q_net2.parameters(), Q_net2.parameters()):\n",
    "        target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data*soft_tau)\n",
    "    \n",
    "    writer.add_scalars('Loss/Q_net_loss', {\"network1\" : Q_loss_1, \"network2\" : Q_loss_2}, update_time)\n",
    "    writer.add_scalar('Loss/alpha_loss', alpha_loss, update_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(totoal_steps, state):\n",
    "    if totoal_steps < start_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            dist, _ = Q_net1(state, alpha)\n",
    "            action = dist.sample().item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Q_network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "totoal_steps = 0\n",
    "update_time = 0\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for i in range(train_steps):\n",
    "        totoal_steps += 1\n",
    "        action = get_action(totoal_steps, state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.store(state, action, next_state, reward, done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done or i==(train_steps-1):\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                sqn_train(update_time)\n",
    "                writer.add_scalar(\"Episode_Steps\", i, update_time)\n",
    "                update_time +=1\n",
    "            break\n",
    "    writer.add_scalar(\"Rewards\", episode_reward, episode)\n",
    "torch.save(Q_net1, model_dir + \"/model.pth\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
