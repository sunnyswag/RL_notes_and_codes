{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Soft Actor-Critic<br>\n",
    "FROM PAPER\n",
    "***\n",
    "**Input:**  $\\theta_1, \\theta_2, \\phi .$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\bar{\\theta}_1\\leftarrow \\theta_1 ,\\bar{\\theta}_2\\leftarrow \\theta_2$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$D \\leftarrow \\varnothing$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** each iteration **do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** each environment step **do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$a_t \\sim \\pi_\\phi(a_t|s_t)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1}\\sim p(s_{t+1}|s_t, a_t)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$D \\leftarrow D\\cup\\{(s_t,a_t,r(s_t,a_t),s_{t+1})\\}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**end for**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** each gradient step **do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_i\\leftarrow\\theta_i - \\lambda_Q\\hat{\\nabla}_{\\theta_i}J_Q(\\theta_i) $&nbsp;&nbsp;for $i\\in \\{1,2\\}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update the Q-function parameters<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi \\leftarrow \\phi - \\lambda_\\pi\\hat{\\nabla}_\\phi J_\\pi(\\phi)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update policy weights<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi\\leftarrow \\lambda_V\\hat{\\nabla}_\\psi J_V(\\psi)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adjust temperature<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\bar{\\theta}_i\\leftarrow \\tau\\theta_i+(1-\\tau)\\bar{\\theta}_i $&nbsp; for$i\\in \\{1,2\\}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update target network weights<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**end for**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**end for**<br>\n",
    "**Output:**&nbsp;$\\theta_1,\\theta_2,\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Formula:\n",
    "1. Soft Bellman residual:\n",
    "$$\n",
    "J_Q(\\theta)=\\mathbb{E}_{(s_t,a_t)\\sim D}\\big[\\frac{1}{2}(Q_\\theta(s_t,a_t)-{\\cal{T}}^\\pi Q(s_t,a_t))^2\\big]\\tag{1}\n",
    "$$\n",
    "Soft Q-value function:\n",
    "$$\n",
    "{\\cal{T}}^\\pi Q(s_t,a_t) \\triangleq r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}[V_\\bar{\\theta}(s_{t+1})]\\tag{2}\n",
    "$$\n",
    "Soft state value function:\n",
    "$$\n",
    "V(s_t) = \\mathbb{E}_{a_t\\sim \\pi}[Q(s_t,a_t)-\\alpha\\log\\pi(a_t|s_t)]\\tag{3}\n",
    "$$\n",
    "由此推出Soft Bellman residual导数:\n",
    "$$\n",
    "\\hat{\\nabla}_\\theta J_Q(\\theta)=\\nabla_\\theta Q_\\theta(a_t,s_t)(Q_\\theta(s_t,a_t)-(r(s_t,a_t)+\\gamma(Q_{\\bar{\\theta}}(s_{t+1},a_{t+1})-\\alpha\\log(\\pi_\\phi(a_{t+1}|s_{t+1}))))\\tag{4}\n",
    "$$\n",
    "2. Policy Loss:\n",
    "$$\n",
    "J_\\pi(\\phi)=-\\mathbb{E}_{s_t\\sim D}\\big[\\mathbb{E}_{a_t\\sim \\pi_\\phi}[Q_\\phi(s_t,a_t)-\\alpha\\log(\\pi_\\phi(a_t|s_t))]\\big]\\tag{5}\n",
    "$$\n",
    "又\n",
    "$$\n",
    "a_t=f_\\phi(\\epsilon_t;s_t),\\tag{6}\n",
    "$$\n",
    "所以可写成:\n",
    "$$\n",
    "J_\\pi(\\phi)=-\\mathbb{E}_{s_t\\sim D,\\;\\epsilon_t\\sim N}[Q_\\theta(s_t,f_\\phi(\\epsilon_t;s_t))-\\alpha\\log\\pi_\\phi(f_\\phi(\\epsilon_t;s_t)|s_t)]\\tag{7}\n",
    "$$\n",
    "所以其导数形式为:\n",
    "$$\n",
    "\\hat{\\nabla}_\\phi J_\\pi(\\phi)=\\nabla_\\phi\\alpha\\log(\\pi_\\phi(a_t|s_t))+\\big(\\nabla_{a_t}\\alpha\\log(\\pi_\\phi(a_t|s_t))-\\nabla_{a_t}Q(s_t,a_t)\\big)\\nabla_\\phi f_\\phi(\\epsilon_t;s_t),\\tag{8}\n",
    "$$\n",
    "3. 自适应temperature $\\alpha$(论文中说$\\alpha$和$Q$、$\\pi$是对偶问题，有点不明白):\n",
    "$$\n",
    "\\alpha^*_t=\\arg {\\min_{\\alpha_t}}\\mathbb{E}_{a_t\\sim\\pi^*_t}[-\\alpha_t\\log\\pi^*_t(a_t|s_t;a_t)-\\alpha_t\\bar{H}]\\tag{9}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula Proofs\n",
    "1. Proof formula $f_2(f_3)$:<br>\n",
    "从最开始的动作转移开始，对于在每次$state$采取的$action$所获得的$soft\\ reward$都可定义如下:<br><br>\n",
    "$$\n",
    "r_{soft}(s_t,a_t)=r(s_t,a_t)+\\gamma\\alpha\\mathbb{E}_{s_{t+1}\\sim \\rho}H(\\pi(\\cdot|s_{t+1}))\\tag{10}\n",
    "$$<br>\n",
    "将其带入到原始的$Q\\ function\\ : Q(s_t,a_t)=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1},a_{t+1}}[Q(s_{t+1},a_{t+1})]$中得:<br><br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{soft}(s_t,a_t)&=r(s_t,a_t)+\\gamma\\alpha\\mathbb{E}_{s_{t+1}\\sim\\rho}H(\\pi(\\cdot|s_{t+1}))+\\gamma\\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})]\\\\\n",
    "&=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho,a_{t+1}\\sim\\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\\gamma\\alpha\\mathbb{E}_{s_{t+1}\\sim\\rho}H(\\pi(\\cdot|s_{t+1}))\\\\\n",
    "&=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho,a_{t+1}\\sim\\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho}\\mathbb{E}_{a_{t+1}\\sim\\pi}[-\\alpha\\log\\pi(a_{t+1}|s_{t+1})]\\\\\n",
    "&=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho}[\\mathbb{E}_{a_{t+1}\\sim\\pi}[Q_{soft}(s_{t+1},a_{t+1})-\\alpha\\log(\\pi(a_{t+1}|s_{t+1}))]]\n",
    "\\end{aligned}\\tag{11}\n",
    "$$\n",
    "2. Proff formula $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [相对熵(KL散度)](https://blog.csdn.net/tsyccnh/article/details/79163834)\n",
    "对于同一个随机变量x单独的概率分布P(x)和Q(x)，用来衡量其分布的差异\n",
    "$$\n",
    "D_{KL}(p||q)=\\sum_{i=1}^n p(x_i)\\log[\\frac{p(x_i)}{q(x_i)}]\n",
    "$$\n",
    "$D_{KL}$越接近于0，$p,q$分布越接近<br><br>\n",
    "展开得\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL}(p||q)&=\\sum_{i=1}^np(x_i)\\log(p(x_i))-\\sum^n_{i+1}p(x_i)\\log(q(x_i)) \\\\\n",
    "&=\\underbrace{-H(p(x))}_{熵}+\\underbrace{[-\\sum^n_{i=1}p(x_i)\\log(q(x_i))]}_{交叉熵}\n",
    "\\end{aligned}\n",
    "$$\n",
    "在分类问题中，假设label为p，则前部分是不变的，即只需计算后部分，即**交叉熵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landscape\n",
    "讲了那么多高深的理论，其实就是在actor和critic计算loss的时候分别考虑了entorpy，增加探索。也就是说critic既然是知道actor前进的，那么你也必须将增加探索这四个字铭记在心，不然你怎么知道actor的动作呢<br>\n",
    "其次，deterministic版本的sac是没有entorpy的，因为是确定性策略所以没有entropy的来源(所以确定性策略的sac其实比TD3效果可能还要差)<br>\n",
    "要有entorpy必须是normal distribution，所以其实要改成离散的动作空间，也就几行代码的事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现的Tips\n",
    "1. 使用torch.no_grad(), 而不是.detach()更加直观"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相较于SAC的五大EDIT\n",
    "\n",
    "1. 将Critic network 改为input state, output Q_value\n",
    "2. 将Actor network 改为softmax输出\n",
    "3. 将Critic更新计算Q改为根据Actor进行gather\n",
    "4. alpha更新通过Actor进行gather\n",
    "5. Actor更新乘上了每个动作的概率，以此来达到方差最小化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Atari环境对学习率非常敏感，1e-3和1e-4差别非常大---<br>\n",
    "加上nn.BatchNorm2d(32)并不能加快学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dims, buffer_size, batch_size):\n",
    "        self.state = np.zeros((buffer_size, state_dims[0], state_dims[1], state_dims[2]), dtype=np.float32)\n",
    "        self.action = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.next_state = np.zeros((buffer_size, state_dims[0], state_dims[1], state_dims[2]), dtype=np.float32)\n",
    "        self.reward = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.done = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.size, self.current_index = 0, 0\n",
    "    \n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        self.state[self.current_index] = state\n",
    "        self.action[self.current_index] = action\n",
    "        self.next_state[self.current_index] = next_state\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index + 1) % self.buffer_size\n",
    "        self.size = min((self.size + 1), self.buffer_size)\n",
    "    \n",
    "    def sample(self):\n",
    "        idx = np.random.choice(self.size, self.batch_size)\n",
    "        return dict(state = torch.FloatTensor(self.state[idx]).to(device),\n",
    "                    action = torch.LongTensor(self.action[idx]).unsqueeze(1).to(device),\n",
    "                    next_state = torch.FloatTensor(self.next_state[idx]).to(device),\n",
    "                    reward = torch.FloatTensor(self.reward[idx]).unsqueeze(1).to(device),\n",
    "                    done = torch.FloatTensor(self.done[idx]).unsqueeze(1).to(device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # 使用 std = $$\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}$$\n",
    "        # 来代替高斯分布(0, std ^ 2)的std\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#         self.common_layer = nn.Sequential(\n",
    "#             nn.Conv2d(state_dims[0], 32, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=1),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "# EDIT 1\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dims, action_dim, hidden_dim=512):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.common_layer = nn.Sequential(\n",
    "            nn.Conv2d(state_dims[0], 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(7 * 7 * 64, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        common = self.common_layer(state)\n",
    "        common = common.view(common.size(0), -1)\n",
    "        \n",
    "        linear = F.relu(self.linear1(common))\n",
    "        value = self.linear2(linear)\n",
    "        return value\n",
    "\n",
    "# EDIT 2\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dims, action_dim, hidden_dim=512):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.common_layer = nn.Sequential(\n",
    "            nn.Conv2d(state_dims[0], 32, kernel_size=8, stride=4),\\\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(7 * 7 * 64, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        common = self.common_layer(state)\n",
    "        common = common.view(common.size(0), -1)\n",
    "        \n",
    "        linear = F.relu(self.linear1(common))\n",
    "        action_probs = F.softmax(self.linear2(linear), dim=1)\n",
    "        max_action_prob = torch.argmax(action_probs, dim=1)\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.logits\n",
    "        \n",
    "        return action.unsqueeze(1), action_probs, action_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state, timesteps):\n",
    "    if start_steps > timesteps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action, _, _ = actor(state)\n",
    "        action = action.item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "start_steps = 10000\n",
    "\n",
    "# env_name = \"Breakout-v0\"\n",
    "# start_steps = 500\n",
    "\n",
    "env = make_atari(env_name)\n",
    "env = wrap_deepmind(env)\n",
    "env = wrap_pytorch(env)\n",
    "\n",
    "algorithm_id = \"soft_actor_critic_discrete_image\"\n",
    "\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 64\n",
    "episodes = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "gamma = 0.99\n",
    "soft_tau = 5e-3\n",
    "actor_update = 2\n",
    "automatic_entropy_tuning = True\n",
    "\n",
    "## hyperparameters\n",
    "\n",
    "current_time = time.strftime('%Y-%m-%d_%H:%M:%S',time.localtime(time.time()))\n",
    "ROOT_DIR = \"../running_log/{}/{}/{}\".format(algorithm_id, env_name, current_time)\n",
    "model_dir = os.path.join(ROOT_DIR, \"model\")\n",
    "plot_dir = os.path.join(ROOT_DIR, \"tensorboard\")\n",
    "os.makedirs(model_dir)\n",
    "os.makedirs(plot_dir)\n",
    "writer = SummaryWriter(plot_dir, comment=\"learning_rate={}-batch_size={}-start_steps={}\"\n",
    "                       .format(learning_rate , batch_size, start_steps))\n",
    "\n",
    "# env = gym.make(env_name)\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "state_dims = env.observation_space.shape\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "critic_1 = Critic(state_dims, action_dim).to(device)\n",
    "critic_2 = Critic(state_dims, action_dim).to(device)\n",
    "target_critic_1 = Critic(state_dims, action_dim).to(device)\n",
    "target_critic_2 = Critic(state_dims, action_dim).to(device)\n",
    "actor = Actor(state_dims, action_dim).to(device)\n",
    "\n",
    "target_critic_1.load_state_dict(critic_1.state_dict())\n",
    "target_critic_2.load_state_dict(critic_2.state_dict())\n",
    "\n",
    "critic_optimizer_1 = optim.Adam(critic_1.parameters(), lr=learning_rate)\n",
    "critic_optimizer_2 = optim.Adam(critic_2.parameters(), lr=learning_rate)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "\n",
    "buffer = ReplayBuffer(state_dims, buffer_size, batch_size)\n",
    "\n",
    "# torch.prod()\n",
    "# Returns the product of all elements in the :attr:`input` tensor\n",
    "# 返回输入张量中所有元素的乘积(可指定维度)\n",
    "if automatic_entropy_tuning:\n",
    "#     target_entropy = - torch.prod(torch.Tensor(env.action_space.shape).to(device)).item() # -4.0\n",
    "    target_entropy = - 1.0\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device) # tensor([0.], device='cuda:0', requires_grad=True)\n",
    "    alpha = log_alpha.exp()\n",
    "    alpha_optim = optim.Adam([log_alpha], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_train(updates, steps_):\n",
    "    global alpha\n",
    "    \n",
    "    for i in range(steps_):\n",
    "        samples = buffer.sample()\n",
    "        state, action, next_state = samples[\"state\"], samples[\"action\"], samples[\"next_state\"]\n",
    "        reward, done = samples[\"reward\"], samples[\"done\"]\n",
    "\n",
    "        # update critic\n",
    "        with torch.no_grad():\n",
    "            # EDIT 3\n",
    "            next_action, _, next_action_log_probs = actor(next_state)\n",
    "            next_action_log_probs = next_action_log_probs.gather(1, next_action.long())\n",
    "            target_Q_1 = target_critic_1(next_state).gather(1, next_action.long())\n",
    "            target_Q_2 = target_critic_2(next_state).gather(1, next_action.long())\n",
    "            Q_target_next = torch.min(target_Q_1, target_Q_2) - alpha * next_action_log_probs\n",
    "            next_q_value = reward + (1.0 - done) * gamma * Q_target_next\n",
    "\n",
    "        Q_1 = critic_1(state).gather(1, action)\n",
    "        Q_2 = critic_2(state).gather(1, action)\n",
    "        critic_loss_1 = F.mse_loss(next_q_value, Q_1)\n",
    "        critic_loss_2 = F.mse_loss(next_q_value, Q_2)\n",
    "\n",
    "        critic_optimizer_1.zero_grad()\n",
    "        critic_loss_1.backward()\n",
    "        critic_optimizer_1.step()\n",
    "\n",
    "        critic_optimizer_2.zero_grad()\n",
    "        critic_loss_2.backward()\n",
    "        critic_optimizer_2.step()\n",
    "\n",
    "        # update actor\n",
    "        # EDIT 5\n",
    "        if i % actor_update == 0:\n",
    "            actions, action_probs, action_log_probs = actor(state)\n",
    "            min_Q_value = torch.min(critic_1(state), critic_2(state))\n",
    "            actor_loss = (alpha * action_log_probs - min_Q_value) * action_probs\n",
    "            actor_loss = torch.sum(actor_loss, dim=1, keepdim=True).mean()\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # update entropy_tuning\n",
    "\n",
    "            if automatic_entropy_tuning:\n",
    "                # EDIT 4\n",
    "                action_log_probs = action_log_probs.gather(1, actions.long())\n",
    "                alpha_loss = - log_alpha * (action_log_probs.detach() + target_entropy)\n",
    "                alpha_loss = alpha_loss.mean()\n",
    "\n",
    "                alpha_optim.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                alpha_optim.step()\n",
    "\n",
    "                alpha = log_alpha.exp()\n",
    "\n",
    "            # update parameter\n",
    "            for target_param, param in zip(target_critic_1.parameters(), critic_1.parameters()):\n",
    "                target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data * soft_tau)\n",
    "            for target_param, param in zip(target_critic_2.parameters(), critic_2.parameters()):\n",
    "                target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data * soft_tau)\n",
    "\n",
    "    writer.add_scalars(\"Loss/Critic\", {\"critic_1\":critic_loss_1,\n",
    "                                \"critic_2\":critic_loss_2}, updates)\n",
    "    writer.add_scalar(\"Loss/Actor\", actor_loss, updates)\n",
    "    writer.add_scalar(\"Loss/Alpha\", alpha_loss, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates, timesteps, done_time = 0, 0, 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for i in count():\n",
    "        timesteps += 1\n",
    "        action = take_action(state, timesteps)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        buffer.store(state, action, next_state, reward, done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            if len(buffer) > batch_size:\n",
    "                sac_train(updates, i+1)\n",
    "                updates += 1\n",
    "            writer.add_scalar(\"Episode_step\", i, done_time)\n",
    "            done_time += 1\n",
    "            break\n",
    "        \n",
    "    writer.add_scalar(\"Reward\", episode_reward, episode)\n",
    "torch.save(actor, model_dir + \"/actor_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "[SOFT ACTOR-CRITIC FOR DISCRETE ACTION SETTINGS](https://arxiv.org/pdf/1910.07207.pdf)\n",
    "\n",
    "[Deep-Reinforcement-Learning-Algorithms-with-PyTorch/agents/actor_critic_agents/SAC_Discrete.py](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/master/agents/actor_critic_agents/SAC_Discrete.py)\n",
    "\n",
    "[https://github.com/Jabberwockyll/deep_rl_ale/blob/master/experience_memory.py](https://github.com/Jabberwockyll/deep_rl_ale/blob/master/experience_memory.py)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
