{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Soft Actor-Critic<br>\n",
    "FROM PAPER\n",
    "***\n",
    "**Input:**  $\\theta_1, \\theta_2, \\phi .$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\bar{\\theta}_1\\leftarrow \\theta_1 ,\\bar{\\theta}_2\\leftarrow \\theta_2$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$D \\leftarrow \\varnothing$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** each iteration **do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** each environment step **do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$a_t \\sim \\pi_\\phi(a_t|s_t)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1}\\sim p(s_{t+1}|s_t, a_t)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$D \\leftarrow D\\cup\\{(s_t,a_t,r(s_t,a_t),s_{t+1})\\}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**end for**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** each gradient step **do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_i\\leftarrow\\theta_i - \\lambda_Q\\hat{\\nabla}_{\\theta_i}J_Q(\\theta_i) $&nbsp;&nbsp;for $i\\in \\{1,2\\}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update the Q-function parameters<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi \\leftarrow \\phi - \\lambda_\\pi\\hat{\\nabla}_\\phi J_\\pi(\\phi)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update policy weights<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi\\leftarrow \\lambda_V\\hat{\\nabla}_\\psi J_V(\\psi)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adjust temperature<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\bar{\\theta}_i\\leftarrow \\tau\\theta_i+(1-\\tau)\\bar{\\theta}_i $&nbsp; for$i\\in \\{1,2\\}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update target network weights<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**end for**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**end for**<br>\n",
    "**Output:**&nbsp;$\\theta_1,\\theta_2,\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Formula:\n",
    "1. Soft Bellman residual:\n",
    "$$\n",
    "J_Q(\\theta)=\\mathbb{E}_{(s_t,a_t)\\sim D}\\big[\\frac{1}{2}(Q_\\theta(s_t,a_t)-{\\cal{T}}^\\pi Q(s_t,a_t))^2\\big]\\tag{1}\n",
    "$$\n",
    "Soft Q-value function:\n",
    "$$\n",
    "{\\cal{T}}^\\pi Q(s_t,a_t) \\triangleq r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}[V_\\bar{\\theta}(s_{t+1})]\\tag{2}\n",
    "$$\n",
    "Soft state value function:\n",
    "$$\n",
    "V(s_t) = \\mathbb{E}_{a_t\\sim \\pi}[Q(s_t,a_t)-\\alpha\\log\\pi(a_t|s_t)]\\tag{3}\n",
    "$$\n",
    "由此推出Soft Bellman residual导数:\n",
    "$$\n",
    "\\hat{\\nabla}_\\theta J_Q(\\theta)=\\nabla_\\theta Q_\\theta(a_t,s_t)(Q_\\theta(s_t,a_t)-(r(s_t,a_t)+\\gamma(Q_{\\bar{\\theta}}(s_{t+1},a_{t+1})-\\alpha\\log(\\pi_\\phi(a_{t+1}|s_{t+1}))))\\tag{4}\n",
    "$$\n",
    "2. Policy Loss:\n",
    "$$\n",
    "J_\\pi(\\phi)=-\\mathbb{E}_{s_t\\sim D}\\big[\\mathbb{E}_{a_t\\sim \\pi_\\phi}[Q_\\phi(s_t,a_t)-\\alpha\\log(\\pi_\\phi(a_t|s_t))]\\big]\\tag{5}\n",
    "$$\n",
    "又\n",
    "$$\n",
    "a_t=f_\\phi(\\epsilon_t;s_t),\\tag{6}\n",
    "$$\n",
    "所以可写成:\n",
    "$$\n",
    "J_\\pi(\\phi)=-\\mathbb{E}_{s_t\\sim D,\\;\\epsilon_t\\sim N}[Q_\\theta(s_t,f_\\phi(\\epsilon_t;s_t))-\\alpha\\log\\pi_\\phi(f_\\phi(\\epsilon_t;s_t)|s_t)]\\tag{7}\n",
    "$$\n",
    "所以其导数形式为:\n",
    "$$\n",
    "\\hat{\\nabla}_\\phi J_\\pi(\\phi)=\\nabla_\\phi\\alpha\\log(\\pi_\\phi(a_t|s_t))+\\big(\\nabla_{a_t}\\alpha\\log(\\pi_\\phi(a_t|s_t))-\\nabla_{a_t}Q(s_t,a_t)\\big)\\nabla_\\phi f_\\phi(\\epsilon_t;s_t),\\tag{8}\n",
    "$$\n",
    "3. 自适应temperature $\\alpha$(论文中说$\\alpha$和$Q$、$\\pi$是对偶问题，有点不明白):\n",
    "$$\n",
    "\\alpha^*_t=\\arg {\\min_{\\alpha_t}}\\mathbb{E}_{a_t\\sim\\pi^*_t}[-\\alpha_t\\log\\pi^*_t(a_t|s_t;a_t)-\\alpha_t\\bar{H}]\\tag{9}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula Proofs\n",
    "1. Proof formula $f_2(f_3)$:<br>\n",
    "从最开始的动作转移开始，对于在每次$state$采取的$action$所获得的$soft\\ reward$都可定义如下:<br><br>\n",
    "$$\n",
    "r_{soft}(s_t,a_t)=r(s_t,a_t)+\\gamma\\alpha\\mathbb{E}_{s_{t+1}\\sim \\rho}H(\\pi(\\cdot|s_{t+1}))\\tag{10}\n",
    "$$<br>\n",
    "将其带入到原始的$Q\\ function\\ : Q(s_t,a_t)=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1},a_{t+1}}[Q(s_{t+1},a_{t+1})]$中得:<br><br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{soft}(s_t,a_t)&=r(s_t,a_t)+\\gamma\\alpha\\mathbb{E}_{s_{t+1}\\sim\\rho}H(\\pi(\\cdot|s_{t+1}))+\\gamma\\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})]\\\\\n",
    "&=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho,a_{t+1}\\sim\\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\\gamma\\alpha\\mathbb{E}_{s_{t+1}\\sim\\rho}H(\\pi(\\cdot|s_{t+1}))\\\\\n",
    "&=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho,a_{t+1}\\sim\\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho}\\mathbb{E}_{a_{t+1}\\sim\\pi}[-\\alpha\\log\\pi(a_{t+1}|s_{t+1})]\\\\\n",
    "&=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim\\rho}[\\mathbb{E}_{a_{t+1}\\sim\\pi}[Q_{soft}(s_{t+1},a_{t+1})-\\alpha\\log(\\pi(a_{t+1}|s_{t+1}))]]\n",
    "\\end{aligned}\\tag{11}\n",
    "$$\n",
    "2. Proff formula $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [相对熵(KL散度)](https://blog.csdn.net/tsyccnh/article/details/79163834)\n",
    "对于同一个随机变量x单独的概率分布P(x)和Q(x)，用来衡量其分布的差异\n",
    "$$\n",
    "D_{KL}(p||q)=\\sum_{i=1}^n p(x_i)\\log[\\frac{p(x_i)}{q(x_i)}]\n",
    "$$\n",
    "$D_{KL}$越接近于0，$p,q$分布越接近<br><br>\n",
    "展开得\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL}(p||q)&=\\sum_{i=1}^np(x_i)\\log(p(x_i))-\\sum^n_{i+1}p(x_i)\\log(q(x_i)) \\\\\n",
    "&=\\underbrace{-H(p(x))}_{熵}+\\underbrace{[-\\sum^n_{i=1}p(x_i)\\log(q(x_i))]}_{交叉熵}\n",
    "\\end{aligned}\n",
    "$$\n",
    "在分类问题中，假设label为p，则前部分是不变的，即只需计算后部分，即**交叉熵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landscape\n",
    "讲了那么多高深的理论，其实就是在actor和critic计算loss的时候分别考虑了entorpy，增加探索。也就是说critic既然是知道actor前进的，那么你也必须将增加探索这四个字铭记在心，不然你怎么知道actor的动作呢<br>\n",
    "其次，deterministic版本的sac是没有entorpy的，因为是确定性策略所以没有entropy的来源(所以确定性策略的sac其实比TD3效果可能还要差)<br>\n",
    "要有entorpy必须是normal distribution，所以其实要改成离散的动作空间，也就几行代码的事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现的Tips\n",
    "1. 使用torch.no_grad(), 而不是.detach()更加直观"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, state_column, action_column):\n",
    "        self.state = np.zeros((buffer_size, state_column), dtype=np.float32)\n",
    "        self.action = np.zeros((buffer_size, action_column), dtype=np.float32)\n",
    "        self.next_state = np.zeros((buffer_size, state_column), dtype=np.float32)\n",
    "        self.reward = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.done = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.size, self.current_index = 0, 0\n",
    "    \n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        self.state[self.current_index] =state\n",
    "        self.action[self.current_index] = action\n",
    "        self.next_state[self.current_index] = next_state\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index + 1) % self.buffer_size\n",
    "        self.size = min((self.size + 1), self.buffer_size)\n",
    "    \n",
    "    def sample(self):\n",
    "        idx = np.random.choice(self.size, self.batch_size)\n",
    "        return dict(state = torch.FloatTensor(self.state[idx]).to(device),\n",
    "                    action = torch.FloatTensor(self.action[idx]).to(device),\n",
    "                    next_state = torch.FloatTensor(self.next_state[idx]).to(device),\n",
    "                    reward = torch.FloatTensor(self.reward[idx]).unsqueeze(1).to(device),\n",
    "                    done = torch.FloatTensor(self.done[idx]).unsqueeze(1).to(device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # 使用 std = $$\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}$$\n",
    "        # 来代替高斯分布(0, std ^ 2)的std\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim+action_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        data_cat = torch.cat((state, action), dim=1)\n",
    "        linear = F.relu(self.linear1(data_cat))\n",
    "        linear = F.relu(self.linear2(linear))\n",
    "        value = self.linear3(linear)\n",
    "        return value\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_space=None, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, hidden_dim).apply(weights_init_)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim).apply(weights_init_)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_dim, action_dim).apply(weights_init_)\n",
    "        self.std_layer  = nn.Linear(hidden_dim, action_dim).apply(weights_init_)\n",
    "        \n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = torch.tensor(1.)\n",
    "            self.action_bias = torch.tensor(0.)\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor((action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor((action_space.high + action_space.low) / 2.)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        common = F.relu(self.linear1(state))\n",
    "        common = F.relu(self.linear2(common))\n",
    "        mean = self.mean_layer(common)\n",
    "        log_std = self.std_layer(common)\n",
    "        log_std = torch.clamp(log_std , min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std.exp()\n",
    "    \n",
    "    def sample(self, state, epsilon=1e-5):\n",
    "        mean, std = self.forward(state)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        normal = Normal(mean, std)\n",
    "        \n",
    "        action_bef = normal.rsample() # reparameterization (mean + std * N(0,1))\n",
    "        \n",
    "        action = torch.tanh(action_bef) * self.action_scale + self.action_bias\n",
    "        \n",
    "        # 获得真实的action的log_prob, 因为action进行了torch.tanh操作\n",
    "        log_prob = normal.log_prob(action_bef)\\\n",
    "                - torch.log(self.action_scale * (1 - torch.tanh(action_bef).pow(2)) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob, mean\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super(Actor, self).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_plot(factor, item, plot_decay):\n",
    "    item_x = np.arange(len(item))\n",
    "    item_smooth = [np.mean(item[i:i+factor]) if i > factor else np.mean(item[0:i+1])\n",
    "                  for i in range(len(item))]\n",
    "    for i in range(len(item)// plot_decay):\n",
    "        item_x = item_x[::2]\n",
    "        item_smooth = item_smooth[::2]\n",
    "    return item_x, item_smooth\n",
    "    \n",
    "def plot(episode, rewards, critic_losses, actor_losses, episode_steps):\n",
    "    clear_output(True)\n",
    "    rewards_x, rewards_smooth = smooth_plot(10, rewards, 600)\n",
    "    critic_losses_x, critic_losses_smooth = smooth_plot(10, critic_losses, 30000)\n",
    "    actor_losses_x, actor_losses_smooth = smooth_plot(10, actor_losses, 30000)\n",
    "    episode_steps_x, episode_steps_smooth = smooth_plot(10, episode_steps, 600)\n",
    "    \n",
    "    plt.figure(figsize=(18, 16))\n",
    "    plt.subplot(411)\n",
    "    plt.title('episode %s. Reward: %s'%(episode, rewards_smooth[-1]))\n",
    "    plt.plot(rewards, label=\"Rewards\", color='lightsteelblue', linewidth='1')\n",
    "    plt.plot(rewards_x, rewards_smooth, label='Smothed_Rewards', color='darkorange', linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.title('Critic_Losses') #%s. Losses: %s'%(episode, critic_losses_smooth[-1]))\n",
    "    plt.plot(critic_losses,label=\"Critic_Losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(critic_losses_x, critic_losses_smooth, \n",
    "             label=\"Smoothed_Critic_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.title('Actor_Losses') #%s. Losses: %s'%(episode, actor_losses_smooth[-1]))\n",
    "    plt.plot(actor_losses,label=\"Actor_Losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(actor_losses_x, actor_losses_smooth, \n",
    "             label=\"Smoothed_Actor_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.title('Episode_Steps %s. Steps: %s'%(episode, episode_steps_smooth[-1]))\n",
    "    plt.plot(episode_steps,label=\"Episode_Steps\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(episode_steps_x, episode_steps_smooth, \n",
    "             label=\"Episode_Steps_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state, updates):\n",
    "    if start_steps > updates:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action, _, _ = actor.sample(state)\n",
    "        action = action.detach().cpu().numpy().flatten()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "env_name = \"BipedalWalker-v2\"\n",
    "# env_name = \"Pendulum-v0\"\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 256\n",
    "episodes = 5000\n",
    "steps = 2000\n",
    "start_steps = 2000\n",
    "learning_rate = 1e-4\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "soft_tau = 5e-3\n",
    "actor_update = 2\n",
    "target_update_interval = 1\n",
    "updates_per_episode = 300\n",
    "automatic_entropy_tuning = True\n",
    "\n",
    "## hyperparameters\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "action_space = env.action_space\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "target_critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "target_critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "actor = Actor(state_dim, action_dim, action_space).to(device)\n",
    "# target_actor = Actor(state_dim, action_dim, action_space).to(device)\n",
    "\n",
    "target_critic_1.load_state_dict(critic_1.state_dict())\n",
    "target_critic_2.load_state_dict(critic_2.state_dict())\n",
    "# target_actor.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic_optimizer_1 = optim.Adam(critic_1.parameters(), lr=learning_rate)\n",
    "critic_optimizer_2 = optim.Adam(critic_2.parameters(), lr=learning_rate)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "\n",
    "buffer = ReplayBuffer(buffer_size, batch_size, state_dim, action_dim)\n",
    "\n",
    "# torch.prod()\n",
    "# Returns the product of all elements in the :attr:`input` tensor\n",
    "# 返回输入张量中所有元素的乘积(可指定维度)\n",
    "if automatic_entropy_tuning:\n",
    "    target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(device)).item() # -4.0\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device) # tensor([0.], device='cuda:0', requires_grad=True)\n",
    "    alpha_optim = optim.Adam([log_alpha], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_train(updates, steps_, alpha):\n",
    "    # for i in range(steps_):\n",
    "    samples = buffer.sample()\n",
    "    state, action, next_state = samples[\"state\"], samples[\"action\"], samples[\"next_state\"]\n",
    "    reward, done = samples[\"reward\"], samples[\"done\"]\n",
    "\n",
    "    # update critic\n",
    "    with torch.no_grad():\n",
    "        # next_action, next_log_pi, _ = target_actor.sample(next_state)\n",
    "        next_action, next_log_pi, _ = actor.sample(next_state)\n",
    "        target_Q_1 = target_critic_1(next_state, next_action)\n",
    "        target_Q_2 = target_critic_2(next_state, next_action)\n",
    "        Q_target_next = torch.min(target_Q_1, target_Q_2) - alpha * next_log_pi\n",
    "        next_q_value = reward + (1.0 - done) * gamma * Q_target_next\n",
    "\n",
    "    Q_1, Q_2 = critic_1(state, action), critic_2(state, action)\n",
    "    critic_loss_1 = F.mse_loss(next_q_value, Q_1)\n",
    "    critic_loss_2 = F.mse_loss(next_q_value, Q_2)\n",
    "\n",
    "    critic_optimizer_1.zero_grad()\n",
    "    critic_loss_1.backward()\n",
    "    critic_optimizer_1.step()\n",
    "\n",
    "    critic_optimizer_2.zero_grad()\n",
    "    critic_loss_2.backward()\n",
    "    critic_optimizer_2.step()\n",
    "\n",
    "    # if i % actor_update == 0:\n",
    "        # update actor\n",
    "\n",
    "    next_action_, log_pi_, _ = actor.sample(state)\n",
    "    min_Q_value = torch.min(critic_1(state, next_action_), critic_2(state, next_action_))\n",
    "    actor_loss = (alpha * log_pi_ - min_Q_value).mean()\n",
    "\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    # update entropy_tuning\n",
    "\n",
    "    if automatic_entropy_tuning:\n",
    "        alpha_loss = - log_alpha * (log_pi_.detach() + target_entropy).mean()\n",
    "\n",
    "        alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        alpha_optim.step()\n",
    "\n",
    "        alpha = log_alpha.exp()\n",
    "\n",
    "    # update parameter\n",
    "    if updates % target_update_interval == 0:\n",
    "#         for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "#             target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data * soft_tau)\n",
    "        for target_param, param in zip(target_critic_1.parameters(), critic_1.parameters()):\n",
    "            target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data * soft_tau)\n",
    "        for target_param, param in zip(target_critic_2.parameters(), critic_2.parameters()):\n",
    "            target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data * soft_tau)\n",
    "                    \n",
    "    return (critic_loss_1 + critic_loss_2).item(), actor_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_losses, actor_losses, all_rewards, episode_steps, updates = [], [], [], [], 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for i in range(steps):\n",
    "        action = take_action(state, updates)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # done = 0 if i+1 == env._max_episode_steps else float(done)\n",
    "        buffer.store(state, action, next_state.flatten(), reward * 5.0, done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        updates += 1\n",
    "        \n",
    "        if done or i==(steps-1):\n",
    "            if len(buffer) > batch_size:\n",
    "                critic_loss, actor_loss = sac_train(updates, i+1, alpha)\n",
    "                critic_losses.append(critic_loss)\n",
    "                actor_losses.append(actor_loss)\n",
    "            episode_steps.append(i)\n",
    "            break\n",
    "        \n",
    "    all_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        plot(episode, all_rewards, critic_losses, actor_losses, episode_steps)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
