{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO, Actor-Critic Style\n",
    "_______________________\n",
    "&nbsp;&nbsp;**for** iteration=1,2,... do<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** actor=1,2,...,N do<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Run policy $\\pi_{\\theta_{old}}$ in environment for T timesteps<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute advantage estimates $\\hat{A}_1,\\dots,\\hat{A}_T$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**end for**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Optimize surrogate(代理人) L wrt $\\theta$,with $K$ epochs and minibatch size $M \\leq NT$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_{old} \\leftarrow \\theta$<br>\n",
    "&nbsp;&nbsp;**end for**\n",
    "_______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function L的数学公式为:\n",
    "$$\n",
    "L_t^{CLIP+VF+S}(\\theta)=\\hat{\\mathbb{E}_t}[L_t^{CLIP}(\\theta)-c_1L^{VF}_t(\\theta)+c_2S[\\pi_\\theta](s_t)]\n",
    "$$\n",
    "其中，$L^{CLIP}(\\theta)=\\hat{\\mathbb{E}_t}\\big[min(r_t(\\theta)\\hat{A}_t,clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon)\\hat{A}_t)\\big]$, $r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$<br>\n",
    "$L^{VF}_t=(V_\\theta(s_t)-V_t^{targ})^2$ **critic loss**<br>\n",
    "S 为奖励熵，保证足够多的探索(写A2C的时候已经OK)<br>\n",
    "$c_1, c_2$为参数\n",
    "#### $L^{CLIP}和r的关系如下$：\n",
    "<img src=\"../assets/PPO_CLIP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE(high-dimensional continuous control using Generalized Advantage Estimation)\n",
    "We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). <br>\n",
    "\n",
    "改进了advantage function的计算方式。将advantage function进行类似于TD(λ)的处理<br>\n",
    "\n",
    "#### 推导过程\n",
    "1. 原始的advantage function : $\\delta^V_t=r_t+\\gamma V(s_{t+1})−V(s_t)$\n",
    "2. $在位置t时,其后k个 \\delta 折扣相加$ : \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{A}^{(1)}_t&:=\\delta^V_t&&=-V(s_t)+r_t+\\gamma V(s_{t+1}) \\\\\n",
    "\\hat{A}^{(2)}_t&:=\\delta^V_t+\\gamma \\delta^V_{t+1}&&=-V(s_t)+r_t+\\gamma r_{t+1}+\\gamma ^2 V(s_{t+2}) \\\\\n",
    "\\hat{A}^{(3)}_t&:=\\delta^V_t+\\gamma \\delta^V_{t+1}+\\gamma^2 \\delta^V_{t+2}&&=-V(s_t)+r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\gamma ^3 V(s_{t+3}) \\\\\n",
    "\\hat{A}_t^{(k)}&:=\\sum_{l=0}^{k=1}\\gamma^l\\delta_{t+l}^V&&=-V(s_t)+r_t+\\gamma r_{t+1}+\\dots+\\gamma^{k-1}r_{t+k-1}+\\gamma^kV(s_{t+k})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "3. $k \\to \\infty, \\gamma^kV(s_{t+k})$会变得非常非常非常小，So :\n",
    "$$\n",
    "\\hat{A}_t^{(\\infty)}=\\sum^\\infty_{l=0}\\gamma^l\\delta_{t+l}^V=-V(s_t)+\\sum^\\infty_{l=0}\\gamma^lr_{t+l}\n",
    "$$\n",
    "4. 所以，$t$ 时刻的GAE可推导为 :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{A}_t^{GAE(\\gamma, \\lambda)}&:=(1-\\lambda)\\big(\\hat{A}_t^{(1)}+\\lambda\\hat{A}_t^{(2)}+\\lambda^2\\hat{A}_t^{(3)}+\\dots\\big)\\\\\n",
    "&=(1-\\lambda)\\big(\\delta_t^V+\\lambda(\\delta_t^V+\\gamma\\delta_{t+1}^V)+\\lambda^2(\\delta_t^V+\\gamma\\delta_{t+1}^V+\\gamma^2\\delta_{t+2}^V)+\\dots\\big)\\\\\n",
    "&=(1-\\lambda)\\big(\\delta^V_t(1+\\lambda+\\lambda^2+\\dots)+\\gamma\\delta^V_{t+1}(\\lambda+\\lambda^2+\\lambda^3+\\dots)+\\gamma^2\\delta^V_{t+2}(\\lambda^2+\\lambda^3+\\lambda^4+\\dots)+\\dots\\big)\\\\\n",
    "&=(1-\\lambda)\\big(\\delta^V_t\\big(\\frac{1}{1-\\lambda}\\big)+\\gamma\\delta^V_{t+1}\\big(\\frac{\\lambda}{1-\\lambda}\\big)+\\gamma^2\\delta^V_{t+2}\\big(\\frac{\\lambda^2}{1-\\lambda}\\big)+\\dots\\big)\\\\\n",
    "&=\\underbrace{\\delta^V_t+\\gamma\\lambda\\delta^V_{t+1}+(\\gamma\\lambda)^2\\delta^V_{t+2}+\\dots}_{此处计算时使用这个公式(迭代计算)}\\\\\n",
    "&=\\sum_{l=0}^\\infty(\\gamma\\lambda)^l\\delta^V_{t+l}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用高斯分布(正态分布)来实现随机性策略控制连续动作空间\n",
    "1. 高斯分布有两个重要的变量一个是均值 $\\mu$ ,另一个是方差 $\\sigma$ 。$\\mu$ 为高斯函数的对称轴，$\\frac{1}{\\sqrt{2\\pi}\\sigma}$ 为高斯函数的最高点。高斯函数的积分为1。所以我们可以使用它来进行连续动作的sample。方差 $\\sigma$ 越大，分布越分散，方差 $\\sigma$ 越小，分布越集中。\n",
    "2. $\\mu$ 的选择很好把控，经过tanh处理之后+简单的数学变换，使nn输出的 $\\mu$ 在env规定的动作空间内就可以\n",
    "3. $\\sigma$ 的选择,使用softplus函数对sigma进行处理。softplus 公式为$f(x)=\\frac{1}{\\beta}log(1+exp(\\beta x))$, softplus 是 ReLU 的平滑近似值版本\n",
    "4. 高斯分布公式:\n",
    "$$\n",
    "f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\bigg(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\bigg)\n",
    "$$\n",
    "5. 和确定性策略相比，需要考虑每个state采取每个动作的概率，计算量确实比较大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRPO\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用第一种和第二种方式来计算TD_target都可行(参见readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_function():\n",
    "    x = np.arange(-10,10,0.05)\n",
    "    plt.figure(figsize=(18,7))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Gaussian distribution\")\n",
    "    mu, sigma = 0, 10\n",
    "    y = lambda x : np.exp(-((x-mu)**2)/(2*sigma**2))/(sigma*np.sqrt(2*np.pi))\n",
    "    plt.plot(x, y(x))\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Softplus\")\n",
    "    y = np.log(1+np.exp(x))\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "\n",
    "plot_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多线程又双叒叕来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(worker_id, master_end, worker_end):\n",
    "    master_end.close()\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    env.seed(worker_id)\n",
    "    \n",
    "    while True:\n",
    "        cmd, data = worker_end.recv()\n",
    "        if cmd == 'step':\n",
    "            state, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            worker_end.send((state, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            state = env.reset()\n",
    "            worker_end.send(state)\n",
    "        elif cmd == 'reset_task':\n",
    "            state = env.reset_task()\n",
    "            worker_end.send(state)\n",
    "        elif cmd == 'close':\n",
    "            worker_end.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            worker_end.send((env.observation_space.shape[0], env.action_space.shape[0]))\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEnv:\n",
    "    def __init__(self, n_train_processes):\n",
    "        self.nenvs = n_train_processes\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        self.workers = []\n",
    "        \n",
    "        self.master_ends, self.worker_ends = zip(*[mp.Pipe() for _ in range(self.nenvs)])\n",
    "        \n",
    "        for worker_id, (master_end, worker_end) in enumerate(zip(self.master_ends, self.worker_ends)):\n",
    "            p = mp.Process(target=worker, args=(worker_id, master_end, worker_end))\n",
    "            p.daemon = False\n",
    "            p.start()\n",
    "            self.workers.append(p)\n",
    "        for worker_end in self.worker_ends:\n",
    "            worker_end.close()\n",
    "        \n",
    "        self.master_ends[0].send(('get_spaces', None))\n",
    "        self.observation_space, self.action_space = self.master_ends[0].recv()\n",
    "        \n",
    "    def step_async(self, actions):\n",
    "        for master_end, action in zip(self.master_ends, actions):\n",
    "            master_end.send(('step', action))\n",
    "        self.waiting = True\n",
    "    \n",
    "    def step_wait(self):\n",
    "        results = [master_end.recv() for master_end in self.master_ends]\n",
    "        self.waiting = False\n",
    "        states, rewards, dones, infos = zip(*results)\n",
    "        return np.stack(states), np.stack(rewards), np.stack(dones), infos\n",
    "    \n",
    "    def reset(self):\n",
    "        for master_end in self.master_ends:\n",
    "            master_end.send(('reset', None))\n",
    "        return np.stack([master_end.recv() for master_end in self.master_ends])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "    \n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            [master_end.recv() for master_end in self.master_ends]\n",
    "        for master_end in self.master_ends:\n",
    "            master_end.send(('close', None))\n",
    "        del self.workers[:]\n",
    "        self.closed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 初始化网络参数信息get\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "class Actor_critic(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, std=0.0):\n",
    "        super(Actor_critic, self).__init__()\n",
    "        self.actor_linear1 = nn.Linear(in_dim, 256)\n",
    "        self.critic_linear1 = nn.Linear(in_dim, 256)\n",
    "        self.actor_linear2 = nn.Linear(256, out_dim)\n",
    "        self.sigma_linear = nn.Linear(256, out_dim)\n",
    "        self.critic_linear2 = nn.Linear(256, 1)\n",
    "#         self.apply(init_weights)\n",
    "        \n",
    "#        self.log_std = nn.Parameter(torch.ones(1, out_dim) * std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value_hidden = F.relu(self.critic_linear1(x))\n",
    "        value = self.actor_linear2(value_hidden)\n",
    "        actor_hidden = F.relu(self.actor_linear1(x))\n",
    "#        mu = 2 * torch.tanh(self.actor_linear2(actor_hidden))\n",
    "#        sigma = F.softplus(self.sigma_linear(actor_hidden)) + 1e-6\n",
    "#        sigma   = self.log_std.exp().expand_as(mu)\n",
    "#        dist = Normal(mu, sigma)\n",
    "        mu = self.actor_linear2(actor_hidden)\n",
    "        dist = F.softmax(mu)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_plot(factor, item, plot_decay):\n",
    "    item_x = np.arange(len(item))\n",
    "    item_smooth = [np.mean(item[i:i+factor]) if i > factor else np.mean(item[0:i+1])\n",
    "                  for i in range(len(item))]\n",
    "    for i in range(len(item)// plot_decay):\n",
    "        item_x = item_x[::2]\n",
    "        item_smooth = item_smooth[::2]\n",
    "    return item_x, item_smooth\n",
    "    \n",
    "def plot(episode, rewards, losses):\n",
    "    clear_output(True)\n",
    "    rewards_x, rewards_smooth = smooth_plot(10, rewards, 200)\n",
    "    losses_x, losses_smooth = smooth_plot(10, losses, 10000)\n",
    "    \n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.title('episode %s. reward: %s'%(episode, rewards_smooth[-1]))\n",
    "    plt.plot(rewards, label=\"Rewards\", color='lightsteelblue', linewidth='1')\n",
    "    plt.plot(rewards_x, rewards_smooth, label='Smothed_Rewards', color='darkorange', linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.title('Losses')\n",
    "    plt.plot(losses,label=\"Losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(losses_x, losses_smooth, \n",
    "             label=\"Smoothed_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def test_env():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).reshape(-1, 3).to(device)\n",
    "        log_prob, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(log_prob.sample().cpu().numpy())\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算GAE<br>\n",
    "对比计算G_t作为无偏估计和reward + gamma * model(next_state)的方差哪个小，再做选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gae_compute(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "#     td_target = next_value\n",
    "#     delta_list = []\n",
    "#     for idx in reversed(range(len(rewards))):\n",
    "#         td_target = td_target * gamma * masks[idx] + rewards[idx]\n",
    "#         delta = td_target - values[idx]\n",
    "#         delta_list.insert(0, delta)\n",
    "    values = values + [next_value]\n",
    "    delta_list = []\n",
    "    for idx in range(len(rewards)):\n",
    "        delta = rewards[idx] + values[idx + 1] * gamma * masks[idx] - values[idx]\n",
    "        delta_list.append(delta)\n",
    "    \n",
    "    advantage = 0\n",
    "    advantage_list = []\n",
    "    for idx in reversed(range(len(delta_list))):\n",
    "        advantage = delta_list[idx] + advantage * gamma * tau\n",
    "        advantage_list.insert(0, advantage)\n",
    "    return advantage_list, delta_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO训练更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, advantages, delta_list):\n",
    "    batch_size = actions.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        ids = np.random.choice(batch_size, mini_batch_size, replace=False)\n",
    "        yield states[ids, :], actions[ids, :], log_probs[ids, :], advantages[ids, :], delta_list[ids, :]\n",
    "    \n",
    "#     batch_size = states.size(0)\n",
    "#     ids = np.random.permutation(batch_size)\n",
    "#     ids = np.split(ids[:batch_size // mini_batch_size * mini_batch_size], batch_size // mini_batch_size)\n",
    "#     for i in range(len(ids)):\n",
    "#         yield states[ids[i], :], actions[ids[i], :], log_probs[ids[i], :], advantages[ids[i], :], delta_list[ids[i], :]\n",
    "\n",
    "def ppo_train(ppo_epochs, mini_batch_size, states, actions, log_probs, advantages, delta_list, clip_param=0.2):\n",
    "    losses = []\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, advantage, delta in ppo_iter(mini_batch_size, states, actions, log_probs,\n",
    "                                                                advantages, delta_list):\n",
    "            dist, _ = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "            \n",
    "            ratio = (new_log_probs - old_log_probs).exp() # 就成了 new_probs / old_probs\n",
    "            sub1 = ratio * advantage\n",
    "            sub2 = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param) * advantage\n",
    "            actor_loss = - torch.min(sub1, sub2).mean()\n",
    "            critic_loss = advantage.pow(2).mean()\n",
    "            \n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return round(mean(losses),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 16\n",
    "envs = ParallelEnv(num_envs)\n",
    "state_space = envs.observation_space\n",
    "action_space = envs.action_space\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Actor_critic(state_space, action_space).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "# input_value = torch.tensor(np.zeros([1,state_space]),dtype=torch.float32).to(device)\n",
    "# writer.add_graph(model, input_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "test_rewards = []\n",
    "loss_list = []\n",
    "ppo_epochs = 40\n",
    "mini_batch_size = 30\n",
    "\n",
    "for epoch in range(10000):\n",
    "    state = envs.reset()\n",
    "    states, actions, rewards, masks, log_probs, values = [], [], [], [], [], []\n",
    "    \n",
    "    for _ in range(20):\n",
    "        dist, value = model(torch.FloatTensor(state).to(device))\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        states.append(torch.FloatTensor(state).to(device))\n",
    "        actions.append(action)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        values.append(value)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "    #为什么需要detach()在这里\n",
    "    _, next_value = model(torch.FloatTensor(next_state).to(device))\n",
    "    advantages, delta_list = gae_compute(next_value, rewards, masks, values)\n",
    "    loss = ppo_train(ppo_epochs, mini_batch_size, torch.cat(states).detach(),torch.cat(actions).detach(), \n",
    "                     torch.cat(log_probs).detach(), torch.cat(advantages).detach(), torch.cat(delta_list).detach())\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        test_reward = np.mean([test_env() for _ in range(10)])\n",
    "        test_rewards.append(test_reward)\n",
    "        plot(epoch + 1, test_rewards, loss_list)\n",
    "#         soft = lambda loss : np.mean(loss[-100:]) if len(loss)>=100 else np.mean(loss)\n",
    "#         writer.add_scalar(\"Test_Rewards\", np.array(soft(test_rewards)), epoch)\n",
    "#         writer.add_scalar(\"Value_Losses\", np.array(soft(loss_list)), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Baselines:\n",
    "<img src=\"../assets/PPO_baseline.png\"></img>\n",
    "### Test_Rewards:\n",
    "<img src=\"../assets/PPO_Test_Rewards.png\" width=100%></img>\n",
    "### Value_Losses:\n",
    "<img src=\"../assets/PPO_Value_Losses.png\"></img>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
