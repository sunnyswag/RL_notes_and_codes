{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q值包含两个信息，一个为所在的位置，另一个为所在位置的所采取的动作\n",
    "$$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$$\n",
    "$$Q^\\pi(s,a)=V^\\pi(s)+A^\\pi(s,a)-\\frac{A^\\pi(s,a)}{N_A}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, column, max_size, batch_size):\n",
    "        self.current_state = np.zeros((max_size, column), dtype=np.float32)\n",
    "        self.next_state = np.zeros((max_size, column), dtype=np.float32)\n",
    "        self.action = np.zeros(max_size, dtype=np.float32)\n",
    "        self.reward = np.zeros(max_size, dtype=np.float32)\n",
    "        self.done = np.zeros(max_size,dtype=np.float32)\n",
    "        self.max_size, self.batch_size = max_size, batch_size\n",
    "        self.size, self.current_index = 0, 0\n",
    "    \n",
    "    def store(self, current_state, action, next_state, reward, done):\n",
    "        self.current_state[self.current_index] = current_state\n",
    "        self.action[self.current_index] = action\n",
    "        self.next_state[self.current_index] = next_state\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        ptr = np.random.choice(self.size, self.batch_size)\n",
    "        return dict(current_state=self.current_state[ptr],\n",
    "                    next_state=self.next_state[ptr],\n",
    "                    action=self.action[ptr],\n",
    "                    reward=self.reward[ptr],\n",
    "                    done=self.done[ptr]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.common_layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage_layers = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "        \n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        common = self.common_layers(x)\n",
    "        advantage = self.advantage_layers(common)\n",
    "        value = self.value_layers(common)\n",
    "        Q = value + advantage - advantage.mean(0, keepdim=True)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f30197341d0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_epsilon = 0.05\n",
    "max_epsilon = 1\n",
    "epsilon_decay = 60\n",
    "epsilon_episode = lambda episode : min_epsilon + np.exp(-episode / epsilon_decay)*0.95\n",
    "plt.plot([epsilon_episode(x) for x in range(600)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed = 777\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)\n",
    "env.seed(seed)        \n",
    "\n",
    "batch_size = 32\n",
    "max_size = 10000\n",
    "memory = ReplayBuffer(env.observation_space.shape[0], max_size, batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "network = Network(env.observation_space.shape[0],env.action_space.n).to(device)\n",
    "target_network = Network(env.observation_space.shape[0],env.action_space.n).to(device)\n",
    "target_network.load_state_dict(network.state_dict())\n",
    "target_network.eval()#把train设置为false，默认为true。可通过target_network.train()将train设置回true\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "gamma = 0.99\n",
    "target_update = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(episode, state):\n",
    "    if np.random.random_sample() > epsilon_episode(episode):\n",
    "        selected_action = network(torch.FloatTensor(state).to(device)).argmax().detach().cpu().numpy()\n",
    "    else:\n",
    "        selected_action = env.action_space.sample()\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    samples = memory.sample_batch()\n",
    "    state = torch.FloatTensor(samples[\"current_state\"]).to(device)\n",
    "    next_state = torch.FloatTensor(samples[\"next_state\"]).to(device)\n",
    "    action = torch.LongTensor(samples[\"action\"].reshape(-1, 1)).to(device)#LongTensor\n",
    "    reward = torch.FloatTensor(samples[\"reward\"].reshape(-1, 1)).to(device)\n",
    "    done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "    \n",
    "    #main changes\n",
    "    current_Q_value = network(state).gather(1, action)\n",
    "    next_action = network(next_state).argmax(1, keepdim=True)\n",
    "    next_Q_value = target_network(next_state).gather(1, next_action)\n",
    "    \n",
    "    target = reward + gamma*next_Q_value*(1 - done)\n",
    "    loss = ((target.detach() - current_Q_value).pow(2)).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(network.parameters(),1.0,norm_type=1) # Gradient clipping(增加稳定性)\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(18,10))\n",
    "    plt.subplot(211)\n",
    "    plt.title('frame %s. mean_reward: %s'%(frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('reward')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(losses)\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-24c2dec9b051>(11)train()\n",
      "-> current_Q_value = network(state).gather(1, action)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  state.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  action.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-24c2dec9b051>(12)train()\n",
      "-> next_action = network(next_state).argmax(1, keepdim=True)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-24c2dec9b051>(13)train()\n",
      "-> next_Q_value = target_network(next_state).gather(1,next_action).detach()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  current_Q_value.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  next_action.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-24c2dec9b051>(15)train()\n",
      "-> target = (reward + gamma*next_Q_value*(1 - done)).to(device)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  next_Q_value.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-24c2dec9b051>(16)train()\n",
      "-> loss = ((target - current_Q_value).pow(2)).mean()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  target.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  target - current_Q_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16388.5000],\n",
      "        [10260.7500],\n",
      "        [37954.4688],\n",
      "        [77684.8438],\n",
      "        [13249.2500],\n",
      "        [14857.6250],\n",
      "        [ 8107.7500],\n",
      "        [14903.6875],\n",
      "        [ -881.6875],\n",
      "        [ -227.8750],\n",
      "        [11348.7500],\n",
      "        [46236.1562],\n",
      "        [ 9091.0000],\n",
      "        [ 9105.8125],\n",
      "        [ 3401.5625],\n",
      "        [14453.5000],\n",
      "        [  540.2500],\n",
      "        [ 8504.8750],\n",
      "        [15830.7500],\n",
      "        [ 9176.0000],\n",
      "        [53605.3203],\n",
      "        [43292.7812],\n",
      "        [ 1680.8750],\n",
      "        [10741.5000],\n",
      "        [13755.6250],\n",
      "        [ 1508.8125],\n",
      "        [67021.0312],\n",
      "        [17671.0000],\n",
      "        [ 8423.0000],\n",
      "        [10338.2500],\n",
      "        [13274.2500],\n",
      "        [ -827.5000]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  target\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1282e+06],\n",
      "        [1.0598e+06],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.4352e+06],\n",
      "        [1.8187e+06],\n",
      "        [1.6082e+06],\n",
      "        [9.8917e+05],\n",
      "        [5.8196e+05],\n",
      "        [3.9010e+05],\n",
      "        [8.8452e+05],\n",
      "        [1.0000e+00],\n",
      "        [1.2000e+06],\n",
      "        [2.6667e+05],\n",
      "        [4.7246e+05],\n",
      "        [2.2762e+06],\n",
      "        [3.0844e+05],\n",
      "        [2.0355e+06],\n",
      "        [2.2012e+06],\n",
      "        [1.6144e+06],\n",
      "        [1.3101e+05],\n",
      "        [1.0000e+00],\n",
      "        [4.5013e+05],\n",
      "        [2.0236e+06],\n",
      "        [1.4762e+06],\n",
      "        [6.4366e+05],\n",
      "        [1.0000e+00],\n",
      "        [2.1302e+06],\n",
      "        [1.9554e+06],\n",
      "        [1.8861e+06],\n",
      "        [2.2624e+06],\n",
      "        [3.3107e+05]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  next_Q_value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2149700.0000],\n",
      "        [1070518.2500],\n",
      "        [  82075.1875],\n",
      "        [  40639.3750],\n",
      "        [1449704.3750],\n",
      "        [1837080.7500],\n",
      "        [1624471.2500],\n",
      "        [ 999155.7500],\n",
      "        [ 587837.5625],\n",
      "        [ 394034.4375],\n",
      "        [ 893452.6250],\n",
      "        [  73261.9375],\n",
      "        [1212169.6250],\n",
      "        [ 269361.0625],\n",
      "        [ 477234.6250],\n",
      "        [2299179.0000],\n",
      "        [ 311558.7500],\n",
      "        [2056054.3750],\n",
      "        [2223466.5000],\n",
      "        [1630708.7500],\n",
      "        [ 132334.0625],\n",
      "        [  76208.1250],\n",
      "        [ 454672.5625],\n",
      "        [2043998.6250],\n",
      "        [1491158.8750],\n",
      "        [ 650161.1875],\n",
      "        [  52062.6250],\n",
      "        [2151719.0000],\n",
      "        [1975197.8750],\n",
      "        [1905142.8750],\n",
      "        [2285249.0000],\n",
      "        [ 334416.9375]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5d0de58d1c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mupdate_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-24c2dec9b051>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnext_Q_value\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_Q_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-24c2dec9b051>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnext_Q_value\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_Q_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "losses = []\n",
    "update_count = 0\n",
    "\n",
    "for episode in range(10000):\n",
    "    state, rewards = env.reset(), 0\n",
    "    for i in range(300):\n",
    "        action = select_action(episode, state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.store(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "        rewards += reward\n",
    "        \n",
    "        if len(memory) > batch_size:\n",
    "            loss = train()\n",
    "            update_count += 1\n",
    "            losses.append(loss)\n",
    "            if update_count % target_update == 0:\n",
    "                target_network.load_state_dict(network.state_dict())\n",
    "                \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    all_rewards.append(rewards)\n",
    "    \n",
    "    if episode % 20 == 0:\n",
    "        plot_reward(episode,all_rewards,losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 发现根本无法收敛哎"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
