{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../assets/maxsqn.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "着实可以理解为soft actor-critic的离散动作版本,但动作sample使用的是Q_network进行softmax操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, column, batch_size, buffer_size):\n",
    "        self.current_state = np.zeros((buffer_size, column), dtype=np.float32)\n",
    "        self.next_state = np.zeros((buffer_size, column), dtype=np.float32)\n",
    "        init = lambda buffer_size : np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.action = init(buffer_size)\n",
    "        self.reward = init(buffer_size)\n",
    "        self.done = init(buffer_size)\n",
    "        self.buffer_size, self.batch_size = buffer_size, batch_size\n",
    "        self.size, self.current_index = 0, 0\n",
    "    \n",
    "    def store(self, current_state, action, next_state, reward, done):\n",
    "        self.current_state[self.current_index] = current_state\n",
    "        self.action[self.current_index] = action\n",
    "        self.next_state[self.current_index] = next_state\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index + 1) % buffer_size\n",
    "        self.size = min(self.buffer_size, (self.size + 1))\n",
    "    \n",
    "    def sample(self):\n",
    "        index = np.random.choice(self.size, self.batch_size, replace=False)\n",
    "        return dict(current_state = torch.tensor(self.current_state[index],dtype=torch.float).to(device),\n",
    "                    action = torch.tensor(self.action[index],dtype=torch.long).reshape(-1, 1).to(device),\n",
    "                    next_state = torch.tensor(self.next_state[index],dtype=torch.float).to(device),\n",
    "                    reward = torch.tensor(self.reward[index]).reshape(-1, 1).to(device),\n",
    "                    done = torch.tensor(self.done[index]).reshape(-1, 1).to(device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=300):\n",
    "        super(Q_network, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state, alpha):\n",
    "        x = self.linear1(state)\n",
    "        x = self.linear2(x)\n",
    "        value = self.linear3(x)\n",
    "        \n",
    "        log_pi = F.softmax(value/alpha, dim=1)\n",
    "        dist = Categorical(log_pi)\n",
    "        \n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"LunarLander-v2\"\n",
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "in_dim = env.observation_space.shape[0]\n",
    "out_dim = env.action_space.n\n",
    "\n",
    "Q_net1 = Q_network(in_dim, out_dim).to(device)\n",
    "Q_net2 = Q_network(in_dim, out_dim).to(device)\n",
    "target_Q_net1 = Q_network(in_dim, out_dim).to(device)\n",
    "target_Q_net2 = Q_network(in_dim, out_dim).to(device)\n",
    "target_Q_net1.load_state_dict(Q_net1.state_dict())\n",
    "target_Q_net2.load_state_dict(Q_net2.state_dict())\n",
    "\n",
    "Q_net1_optimizer = optim.Adam(Q_net1.parameters())\n",
    "Q_net2_optimizer = optim.Adam(Q_net2.parameters())\n",
    "\n",
    "train_episodes = 5000\n",
    "train_steps = 500\n",
    "start_steps = 100\n",
    "alpha = 0.15\n",
    "\n",
    "buffer_size = int(1.5e4)\n",
    "batch_size = 128\n",
    "replay_buffer = ReplayBuffer(in_dim, batch_size, buffer_size)\n",
    "\n",
    "# target_entropy = 0.1\n",
    "# log_alpha = torch.zeros(1, requires_grad=True, device=device) # tensor([0.], device='cuda:0', requires_grad=True)\n",
    "# alpha = torch.exp(log_alpha)\n",
    "# alpha_optim = optim.Adam([log_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_plot(factor, item, plot_decay):\n",
    "    item_x = np.arange(len(item))\n",
    "    item_smooth = [np.mean(item[i:i+factor]) if i > factor else np.mean(item[0:i+1])\n",
    "                  for i in range(len(item))]\n",
    "    for i in range(len(item)// plot_decay):\n",
    "        item_x = item_x[::2]\n",
    "        item_smooth = item_smooth[::2]\n",
    "    return item_x, item_smooth\n",
    "    \n",
    "def plot(episode, rewards, critic_losses, actor_losses, episode_steps):\n",
    "    clear_output(True)\n",
    "    rewards_x, rewards_smooth = smooth_plot(10, rewards, len(rewards)//10)\n",
    "    critic_losses_x, critic_losses_smooth = smooth_plot(10, critic_losses, len(critic_losses)//10)\n",
    "    actor_losses_x, actor_losses_smooth = smooth_plot(10, actor_losses, len(actor_losses)//10)\n",
    "    episode_steps_x, episode_steps_smooth = smooth_plot(10, episode_steps, len(episode_steps)//10)\n",
    "    \n",
    "    plt.figure(figsize=(18, 16))\n",
    "    plt.subplot(411)\n",
    "    plt.title('episode %s. Reward: %s'%(episode, rewards_smooth[-1]))\n",
    "    plt.plot(rewards, label=\"Rewards\", color='lightsteelblue', linewidth='1')\n",
    "    plt.plot(rewards_x, rewards_smooth, label='Smothed_Rewards', color='darkorange', linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.title('Q_net1_losses') #%s. Losses: %s'%(episode, critic_losses_smooth[-1]))\n",
    "    plt.plot(critic_losses,label=\"Q_net1_losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(critic_losses_x, critic_losses_smooth, \n",
    "             label=\"Smoothed_Q_net1_losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.title('Alpha_losses') #%s. Losses: %s'%(episode, actor_losses_smooth[-1]))\n",
    "    plt.plot(actor_losses,label=\"Alpha_losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(actor_losses_x, actor_losses_smooth, \n",
    "             label=\"Smoothed_Alpha_losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.title('Episode_Steps %s. Steps: %s'%(episode, episode_steps_smooth[-1]))\n",
    "    plt.plot(episode_steps,label=\"Episode_Steps\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(episode_steps_x, episode_steps_smooth, \n",
    "             label=\"Episode_Steps_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqn_train(gamma=0.99, soft_tau=0.1):\n",
    "    global alpha, Q_net1, Q_net2, target_Q_net1, target_Q_net2\n",
    "    samples = replay_buffer.sample()\n",
    "    state, action, next_state = samples['current_state'], samples['action'], samples['next_state']\n",
    "    reward, done = samples['reward'], samples['done']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        target_dist_1, target_value_1 = target_Q_net1(next_state, alpha)\n",
    "#         entropy_1 = target_dist_1.entropy().unsqueeze(1)\n",
    "        _, target_value_2 = target_Q_net2(next_state, alpha)\n",
    "        target_value = torch.min(\n",
    "                            torch.max(target_value_1, dim=1, keepdim=True)[0],\n",
    "                            torch.max(target_value_2, dim=1, keepdim=True)[0]\n",
    "                        )\n",
    "        target_dist, _ = Q_net1(next_state, alpha)\n",
    "        target_entropy_1 = target_dist.entropy().unsqueeze(1)\n",
    "        target_Q = reward + gamma * (1-done) * (target_value - alpha * target_entropy_1)\n",
    "    \n",
    "    _, value1 = Q_net1(state, alpha)\n",
    "    _, value2 = Q_net2(state, alpha)\n",
    "    Q_value_1 = torch.gather(value1, 1, action)\n",
    "    Q_value_2 = torch.gather(value2, 1, action)\n",
    "    \n",
    "    Q_loss_1 = F.mse_loss(target_Q, Q_value_1)\n",
    "    Q_loss_2 = F.mse_loss(target_Q, Q_value_2)\n",
    "    \n",
    "    Q_net1_optimizer.zero_grad()\n",
    "    Q_loss_1.backward()\n",
    "    Q_net1_optimizer.step()\n",
    "    \n",
    "    Q_net2_optimizer.zero_grad()\n",
    "    Q_loss_2.backward()\n",
    "    Q_net2_optimizer.step()\n",
    "    \n",
    "#     alpha_loss = - log_alpha * (entropy_1 + target_entropy).mean()\n",
    "\n",
    "#     alpha_optim.zero_grad()\n",
    "#     alpha_loss.backward()\n",
    "#     alpha_optim.step()\n",
    "\n",
    "#     alpha = log_alpha.exp()\n",
    "    \n",
    "    for target_param, param in zip(target_Q_net1.parameters(), Q_net1.parameters()):\n",
    "        target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data*soft_tau)\n",
    "    for target_param, param in zip(target_Q_net2.parameters(), Q_net2.parameters()):\n",
    "        target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data*soft_tau)\n",
    "    \n",
    "    return Q_loss_1.item()# , alpha_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(totoal_steps, state):\n",
    "    global Q_net1\n",
    "    if totoal_steps < start_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            dist, _ = Q_net1(state, alpha)\n",
    "            action = dist.sample().item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-626e8ce6b143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mall_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_net1_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2b8f973844e7>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(episode, rewards, critic_losses, actor_losses, episode_steps)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrewards_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcritic_losses_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_losses_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mactor_losses_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_losses_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2b8f973844e7>\u001b[0m in \u001b[0;36msmooth_plot\u001b[0;34m(factor, item, plot_decay)\u001b[0m\n\u001b[1;32m      3\u001b[0m     item_smooth = [np.mean(item[i:i+factor]) if i > factor else np.mean(item[0:i+1])\n\u001b[1;32m      4\u001b[0m                   for i in range(len(item))]\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m \u001b[0mplot_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mitem_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mitem_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_smooth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "Q_net1_losses, alpha_losses, all_rewards, episode_steps = [], [], [], []\n",
    "totoal_steps = 0\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for i in range(train_steps):\n",
    "        totoal_steps += 1\n",
    "        action = get_action(totoal_steps, state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.store(state, action, next_state, reward, done)      \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        if done or i==(train_steps-1):\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                critic_loss, alpha_loss = sqn_train()\n",
    "                Q_net1_losses.append(critic_loss)\n",
    "                alpha_losses.append(alpha_loss)\n",
    "            episode_steps.append(i)\n",
    "            break\n",
    "    \n",
    "    all_rewards.append(episode_reward)\n",
    "    if episode % 20 == 0:\n",
    "        plot(episode, all_rewards, Q_net1_losses, alpha_losses, episode_steps)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
