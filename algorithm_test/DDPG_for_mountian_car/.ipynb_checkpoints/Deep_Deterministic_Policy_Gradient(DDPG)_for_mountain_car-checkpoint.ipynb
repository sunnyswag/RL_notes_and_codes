{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "from replay_buffer import ReplayBuffer\n",
    "from normalize_action import NormalizeActions\n",
    "from noise import OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.linear_state = nn.Linear(state_dim, 64)\n",
    "        self.linear_action = nn.Linear(action_dim, 64)\n",
    "        self.linear2 = nn.Linear(128, 32)\n",
    "        self.linear3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        hidden_state = F.relu(self.linear_state(state))\n",
    "        hidden_action = F.relu(self.linear_action(action))\n",
    "        cat_state_action = torch.cat((hidden_action, hidden_state),dim=1)\n",
    "        hidden2 = F.relu(self.linear2(cat_state_action))\n",
    "        Q = self.linear3(hidden2)\n",
    "        return Q\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, out_dim) # (256, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state,dtype=torch.float).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, ):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ddpg_train(batch_size, gamma=0.99, soft_tau=1e-2):\n",
    "    samples = replay_buffer.sample()\n",
    "    state, action, next_state = samples['current_state'], samples['action'], samples['next_state']\n",
    "    reward, done = samples['reward'], samples['done']\n",
    "    \n",
    "    target_value = reward + (1.0-done)*gamma*target_value_net(next_state, target_policy_net(next_state))\n",
    "    value = value_net(state, action)\n",
    "    value_loss = ((value - target_value.detach()).pow(2)).mean()\n",
    "    \n",
    "    policy_loss = -value_net(state, policy_net(state)).mean()\n",
    "    \n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data*soft_tau)\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "        target_param.data.copy_(target_param.data*(1.0-soft_tau) + param.data*soft_tau)\n",
    "    \n",
    "    return value_loss.item(), policy_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = NormalizeActions(gym.make(\"Pendulum-v0\"))\n",
    "env = NormalizeActions(gym.make(\"MountainCarContinuous-v0\"))\n",
    "ou_noise = OUNoise(env.action_space)\n",
    "\n",
    "in_dim = env.observation_space.shape[0] # 3\n",
    "out_dim = env.action_space.shape[0] # 1 连续动作空间\n",
    "\n",
    "value_net = ValueNetwork(in_dim, out_dim).to(device)\n",
    "policy_net = PolicyNetwork(in_dim, out_dim).to(device)\n",
    "\n",
    "target_value_net = ValueNetwork(in_dim, out_dim).to(device)\n",
    "target_policy_net = PolicyNetwork(in_dim, out_dim).to(device)\n",
    "target_value_net.load_state_dict(value_net.state_dict())\n",
    "target_policy_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "value_optimizer = optim.Adam(value_net.parameters())\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "train_episodes = 250\n",
    "train_steps = 1000\n",
    "test_episodes = int(train_episodes / 2)\n",
    "test_steps = 100\n",
    "\n",
    "buffer_size = 1000000\n",
    "batch_size = 128\n",
    "replay_buffer = ReplayBuffer(in_dim, batch_size, buffer_size)\n",
    "\n",
    "test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def smooth_plot(factor, item, plot_decay):\n",
    "    item_x = np.arange(len(item))\n",
    "    item_smooth = [np.mean(item[i:i+factor]) if i > factor else np.mean(item[0:i+1])\n",
    "                  for i in range(len(item))]\n",
    "    for i in range(len(item)// plot_decay):\n",
    "        item_x = item_x[::2]\n",
    "        item_smooth = item_smooth[::2]\n",
    "    return item_x, item_smooth\n",
    "    \n",
    "def plot(episode, rewards, value_losses, policy_losses, noise):\n",
    "    clear_output(True)\n",
    "    rewards_x, rewards_smooth = smooth_plot(10, rewards, 500)\n",
    "    value_losses_x, value_losses_smooth = smooth_plot(10, value_losses, 10000)\n",
    "    policy_losses_x, policy_losses_smooth = smooth_plot(10, policy_losses, 10000)\n",
    "    noise_x, noise_smooth = smooth_plot(10, noise, 100)\n",
    "    \n",
    "    plt.figure(figsize=(18, 12))\n",
    "    plt.subplot(411)\n",
    "    plt.title('episode %s. reward: %s'%(episode, rewards_smooth[-1]))\n",
    "    plt.plot(rewards, label=\"Rewards\", color='lightsteelblue', linewidth='1')\n",
    "    plt.plot(rewards_x, rewards_smooth, label='Smothed_Rewards', color='darkorange', linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.title('Value_Losses')\n",
    "    plt.plot(value_losses,label=\"Value_Losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(value_losses_x, value_losses_smooth, \n",
    "             label=\"Smoothed_Value_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.title('Policy_Losses')\n",
    "    plt.plot(policy_losses,label=\"Policy_Losses\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(policy_losses_x, policy_losses_smooth, \n",
    "             label=\"Smoothed_Policy_Losses\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.title('Noise')\n",
    "    plt.plot(noise,label=\"Noise\",color='lightsteelblue',linewidth='1')\n",
    "    plt.plot(noise_x, noise_smooth, \n",
    "             label=\"Smoothed_Noise\",color='darkorange',linewidth='3')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# value_losses = []\n",
    "# policy_losses = []\n",
    "# all_rewards = []\n",
    "# updates = 0\n",
    "# test = True\n",
    "\n",
    "# for episode in range(train_episodes):\n",
    "#     state = env.reset()\n",
    "#     ou_noise.reset()\n",
    "#     episode_reward = 0\n",
    "#     noises = []\n",
    "#     for step in range(train_steps):\n",
    "#         action1 = policy_net.get_action(state)\n",
    "        \n",
    "# #         action = ou_noise.get_action(action1, step)\n",
    "# #         noises.append(action[0][0]-action1[0][0])\n",
    "        \n",
    "#         # 200 update in 10\n",
    "#         if step % 200 == 0:\n",
    "#             test = not test\n",
    "#         noise = abs(np.random.randn(1)) if test else -abs(np.random.randn(1))\n",
    "#         action = action1 + noise\n",
    "#         noises.append(noise)\n",
    "        \n",
    "#         next_state, reward, done, _ = env.step(action.flatten())\n",
    "\n",
    "#         replay_buffer.store(state, action, next_state.flatten(), reward, done)\n",
    "#         if len(replay_buffer) > batch_size :\n",
    "#             value_loss, policy_loss = ddpg_train(batch_size)\n",
    "#             value_losses.append(value_loss)\n",
    "#             policy_losses.append(policy_loss)\n",
    "\n",
    "#         state = next_state\n",
    "#         episode_reward += reward\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "        \n",
    "#         updates += 1\n",
    "    \n",
    "#     all_rewards.append(episode_reward)\n",
    "\n",
    "#     plot(episode, all_rewards, value_losses, policy_losses, noises[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_ddpg(time, writer, update_step, noise_discount):\n",
    "    \n",
    "    test = True\n",
    "    \n",
    "    for episode in range(train_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        for step in range(train_steps):\n",
    "            action1 = policy_net.get_action(state)\n",
    "    \n",
    "            if step % update_step == 0:\n",
    "                test = not test\n",
    "            noise_sample = abs(np.random.randn(1)) * noise_discount\n",
    "            noise = noise_sample if test else -noise_sample\n",
    "            action = action1 + noise\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action.flatten())\n",
    "\n",
    "            replay_buffer.store(state, action, next_state.flatten(), reward, done)\n",
    "            if len(replay_buffer) > batch_size :\n",
    "                value_loss, policy_loss = ddpg_train(batch_size)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        writer.add_scalars(\"train_reward/update_step_{}\".format(update_step),\n",
    "                           {\"noise_discount_{}\".format(noise_discount):episode_reward}, episode)\n",
    "        \n",
    "    torch.save(policy_net.state_dict(), \"./test/Continue_time_{}/model/update_step_{}_noise_discount_{}.pth\".format(time, update_step, noise_discount))\n",
    "    print(\"Train < update_step : {}, noise_discount : {} > finished !\".format(update_step, noise_discount))\n",
    "\n",
    "def test_ddpg(time, writer, update_step, noise_discount):\n",
    "    policy_net_1 = PolicyNetwork(in_dim, out_dim).to(device)\n",
    "    policy_net_1.load_state_dict(torch.load(\"./test/Continue_time_{}/model/update_step_{}_noise_discount_{}.pth\".format(time, update_step, noise_discount)))\n",
    "    for test_episode in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        for _ in range(test_steps):\n",
    "            action = policy_net_1.get_action(state.flatten())\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            rewards += reward\n",
    "            if done: break\n",
    "        writer.add_scalars(\"test_reward/update_step_{}\".format(update_step),\n",
    "                           {\"noise_discount_{}\".format(noise_discount):rewards}, test_episode)\n",
    "    print(\"Test  < update_step : {}, noise_discount : {} > finished !\".format(update_step, noise_discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(policy_net.state_dict(), \"./model/DDPG_for_mountain_car.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# policy_net_1 = PolicyNetwork(in_dim, out_dim).to(device)\n",
    "# policy_net_1.load_state_dict(torch.load(\"./model/DDPG_for_mountain_car.pth\"))\n",
    "# policy_net_1.eval()\n",
    "\n",
    "# import pdb\n",
    "# import gym\n",
    "# from IPython import display\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# env = gym.make(\"MountainCarContinuous-v0\")\n",
    "# state = env.reset()\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "# for _ in range(1000):\n",
    "#     img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "#     display.display(plt.gcf())\n",
    "#     display.clear_output(wait=True)\n",
    "#     policy_net = policy_net.cpu()\n",
    "    \n",
    "#     action = policy_net(torch.FloatTensor(state)).detach().numpy()\n",
    "#     # action = env.action_space.sample()\n",
    "#     next_state, _, done, _ = env.step(action)\n",
    "#     if done: \n",
    "#         state = env.reset()\n",
    "#     state = next_state\n",
    "    \n",
    "# from gym import wrappers\n",
    "\n",
    "# env = gym.make(\"MountainCarContinuous-v0\")\n",
    "# env = wrappers.Monitor(env, \"./gym-results/DDPG_mountaincar/\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot use tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train < update_step : 50, noise_discount : 0.1 > finished !\n",
      "Test  < update_step : 50, noise_discount : 0.1 > finished !\n",
      "Train < update_step : 50, noise_discount : 0.5 > finished !\n",
      "Test  < update_step : 50, noise_discount : 0.5 > finished !\n",
      "Train < update_step : 50, noise_discount : 1.0 > finished !\n",
      "Test  < update_step : 50, noise_discount : 1.0 > finished !\n",
      "Train < update_step : 50, noise_discount : 1.5 > finished !\n",
      "Test  < update_step : 50, noise_discount : 1.5 > finished !\n",
      "Train < update_step : 50, noise_discount : 2.0 > finished !\n",
      "Test  < update_step : 50, noise_discount : 2.0 > finished !\n",
      "Train < update_step : 50, noise_discount : 3.0 > finished !\n",
      "Test  < update_step : 50, noise_discount : 3.0 > finished !\n",
      "Train < update_step : 100, noise_discount : 0.1 > finished !\n",
      "Test  < update_step : 100, noise_discount : 0.1 > finished !\n",
      "Train < update_step : 100, noise_discount : 0.5 > finished !\n",
      "Test  < update_step : 100, noise_discount : 0.5 > finished !\n",
      "Train < update_step : 100, noise_discount : 1.0 > finished !\n",
      "Test  < update_step : 100, noise_discount : 1.0 > finished !\n",
      "Train < update_step : 100, noise_discount : 1.5 > finished !\n",
      "Test  < update_step : 100, noise_discount : 1.5 > finished !\n",
      "Train < update_step : 100, noise_discount : 2.0 > finished !\n",
      "Test  < update_step : 100, noise_discount : 2.0 > finished !\n",
      "Train < update_step : 100, noise_discount : 3.0 > finished !\n",
      "Test  < update_step : 100, noise_discount : 3.0 > finished !\n",
      "Train < update_step : 150, noise_discount : 0.1 > finished !\n",
      "Test  < update_step : 150, noise_discount : 0.1 > finished !\n",
      "Train < update_step : 150, noise_discount : 0.5 > finished !\n",
      "Test  < update_step : 150, noise_discount : 0.5 > finished !\n",
      "Train < update_step : 150, noise_discount : 1.0 > finished !\n",
      "Test  < update_step : 150, noise_discount : 1.0 > finished !\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cfb02f4ac32d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mrun_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_discount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1cc13a9a7dbb>\u001b[0m in \u001b[0;36mrun_ddpg\u001b[0;34m(time, writer, update_step, noise_discount)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-be07af4bbc30>\u001b[0m in \u001b[0;36mddpg_train\u001b[0;34m(batch_size, gamma, soft_tau)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mvalue_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalue_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time = time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n",
    "writer = SummaryWriter(log_dir=\"test/Continue_time_{}/tensorboard/\".format(time))\n",
    "\n",
    "dirName = \"./test/Continue_time_{}/model\".format(time)\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "\n",
    "update_steps = [50, 100, 150, 200, 250, 300]\n",
    "noise_discounts = [0.1, 0.5, 1.0, 1.5, 2.0, 3.0]\n",
    "\n",
    "\n",
    "for update_step in update_steps:\n",
    "    for noise_discount in noise_discounts:\n",
    "        \n",
    "        # train\n",
    "        \n",
    "        run_ddpg(time, writer, update_step, noise_discount)\n",
    "        \n",
    "        # test\n",
    "        \n",
    "        test_ddpg(time, writer, update_step, noise_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot use seaborn and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# train_reward = np.array([])\n",
    "# test_reward = np.array([])\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     # train\n",
    "    \n",
    "#     rewards = run_ddpg(epoch)\n",
    "#     train_reward = np.concatenate((train_reward, rewards))\n",
    "    \n",
    "#     # test\n",
    "    \n",
    "#     rewards = test_ddpg(epoch)\n",
    "#     test_reward = np.concatenate((test_reward, rewards))\n",
    "    \n",
    "# train_x = lambda : np.arange(1, train_episodes+1)\n",
    "# train_list = np.stack((train_x() for _ in range(5)), axis=0).flatten()\n",
    "    \n",
    "# test_x = lambda : np.arange(1, test_episodes+1)\n",
    "# test_list = np.stack((test_x() for _ in range(5)), axis=0).flatten()\n",
    "\n",
    "# train_data = pd.DataFrame(dict(x=train_list, y=train_reward))\n",
    "# test_data = pd.DataFrame(dict(x=test_list, y=test_reward))\n",
    "\n",
    "# train_data.to_csv(\"./test/DDPG_OUnoise/trian_data.csv\", index=False)\n",
    "# test_data.to_csv(\"./test/DDPG_OUnoise/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns; sns.set()\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# DDPG_NO_test_data = pd.read_csv(\"RL_notes_and_codes/algorithm_implement/test/DDPG_Normalnoise/test_data.csv\")\n",
    "# DDPG_NO_train_data = pd.read_csv(\"RL_notes_and_codes/algorithm_implement/test/DDPG_Normalnoise/trian_data.csv\")\n",
    "# DDPG_OU_test_data = pd.read_csv(\"RL_notes_and_codes/algorithm_implement/test/DDPG_OUnoise/test_data.csv\")\n",
    "# DDPG_OU_train_data = pd.read_csv(\"RL_notes_and_codes/algorithm_implement/test/DDPG_OUnoise/trian_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG_NO_test_data[\"diff\"] = \"Normalnoise\"\n",
    "# DDPG_NO_train_data[\"diff\"] = \"Normalnoise\"\n",
    "# DDPG_OU_test_data[\"diff\"] = \"OUnoise\"\n",
    "# DDPG_OU_train_data[\"diff\"] = \"OUnoise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG_test_data = pd.concat((DDPG_NO_test_data, DDPG_OU_test_data))\n",
    "# DDPG_train_data = pd.concat((DDPG_NO_train_data, DDPG_OU_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 8))\n",
    "\n",
    "# plt.subplot(211)\n",
    "# ax = sns.lineplot(x=\"x\", y=\"y\", hue=\"diff\", data=DDPG_test_data)\n",
    "# plt.title(\"Test 5 times Reward of 100 steps for each episode, Avg: Normalnoise {}, OUnoise {}\"\n",
    "#          .format(round(DDPG_test_data[DDPG_test_data['diff']==\"Normalnoise\"]['y'].mean(), 3), \n",
    "#                 round(DDPG_test_data[DDPG_test_data['diff']==\"OUnoise\"]['y'].mean(), 3)))\n",
    "# plt.xlabel(\"episodes\")\n",
    "# plt.ylabel(\"rewards\")\n",
    "\n",
    "# plt.subplot(212)\n",
    "# ax = sns.lineplot(x=\"x\", y=\"y\", hue=\"diff\", data=DDPG_train_data)\n",
    "# plt.title(\"Train 5 times Reward of 1000000 steps for each episode, Avg: Normalnoise {}, OUnoise {}\"\n",
    "#          .format(round(DDPG_train_data[DDPG_train_data['diff']==\"Normalnoise\"]['y'].mean(), 3), \n",
    "#                 round(DDPG_train_data[DDPG_train_data['diff']==\"OUnoise\"]['y'].mean(), 3)))\n",
    "# plt.xlabel(\"episodes\")\n",
    "# plt.ylabel(\"rewards\")\n",
    "\n",
    "# plt.savefig(\"RL_notes_and_codes/algorithm_implement/test/Noise_test.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ddpg_noise_test.png](../assets/ddpg_noise_test_1.png)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 691.0904539999999,
   "position": {
    "height": "712.525px",
    "left": "1171.09px",
    "right": "20px",
    "top": "194px",
    "width": "603.05px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
